{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "TextBlob.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyadeepDebnath/NLP_Libraries/blob/master/TextBlob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G3yRNOeMKmG",
        "colab_type": "text"
      },
      "source": [
        "# **TextBlob Operations - NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EK3rFru-Bn2",
        "colab_type": "text"
      },
      "source": [
        "Contains functionalities for the tasks like :-\n",
        "1. Data Preprocessing Tasks (Tokenization, Word Inflection and Lemmatization, Language Detection and Translation, Spelling Correction, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlG8g2p0MhGk",
        "colab_type": "text"
      },
      "source": [
        "### **Setup in Colab Notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dhI2xLaIUFK",
        "colab_type": "code",
        "outputId": "d0f4dd39-bad7-4d54-bb9a-ed386e10fa49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "\n",
        "text = \"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages. In particular, how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\"\n",
        "paragraph = TextBlob(text)\n",
        "#printing the paragraph\n",
        "print(\"\\nParagraph - \" + str(paragraph))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "\n",
            "Paragraph - Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages. In particular, how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDjpNjatL3IG",
        "colab_type": "text"
      },
      "source": [
        "### **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahM8pn1GIUFR",
        "colab_type": "code",
        "outputId": "3cb73e17-c833-4013-ad06-a42566f73abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "#printing all sentences\n",
        "print(\"\\nAll Sentences - \" + str(paragraph.sentences))\n",
        "#printing first sentence\n",
        "print(\"\\nFirst Sentence - \" + str(paragraph.sentences[0]) + \"\\nLast Sentence - \" + str(paragraph.sentences[-1]))\n",
        "#printing all words\n",
        "print(\"\\nAll Words - \" + str(paragraph.words))\n",
        "#printing first word\n",
        "print(\"\\nFirst Word - \" + paragraph.words[0] + \"\\nLast Word - \" + paragraph.words[-1])\n",
        "#printing words of first sentence\n",
        "print(\"\\nWords of One Sentence (one by one) :-\")\n",
        "for word in paragraph.sentences[0].words:\n",
        "    print(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "All Sentences - [Sentence(\"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages.\"), Sentence(\"In particular, how to program computers to process and analyze large amounts of natural language data.\"), Sentence(\"Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\")]\n",
            "\n",
            "First Sentence - Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages.\n",
            "Last Sentence - Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
            "\n",
            "All Words - ['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'subfield', 'of', 'linguistics', 'computer', 'science', 'information', 'engineering', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'In', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', 'natural', 'language', 'understanding', 'and', 'natural', 'language', 'generation']\n",
            "\n",
            "First Word - Natural\n",
            "Last Word - generation\n",
            "\n",
            "Words of One Sentence (one by one) :-\n",
            "Natural\n",
            "language\n",
            "processing\n",
            "NLP\n",
            "is\n",
            "a\n",
            "subfield\n",
            "of\n",
            "linguistics\n",
            "computer\n",
            "science\n",
            "information\n",
            "engineering\n",
            "and\n",
            "artificial\n",
            "intelligence\n",
            "concerned\n",
            "with\n",
            "the\n",
            "interactions\n",
            "between\n",
            "computers\n",
            "and\n",
            "human\n",
            "natural\n",
            "languages\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fd9zcJ4K06z",
        "colab_type": "text"
      },
      "source": [
        "### **Noun Phrase Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H856ShCZK-Cy",
        "colab_type": "code",
        "outputId": "db16511a-59a1-4b96-8a1e-c42209a421f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(\"All Noun Phrases - \" + str(paragraph.noun_phrases))\n",
        "print(\"\\nNoun Phrases of One Sentence (one by one) :-\")\n",
        "for np in paragraph.sentences[0].noun_phrases:\n",
        "    print(np)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Noun Phrases - ['natural language processing', 'nlp', 'computer science', 'information engineering', 'artificial intelligence', 'program computers', 'large amounts', 'natural language data', 'challenges', 'natural language processing', 'speech recognition', 'natural language understanding', 'natural language generation']\n",
            "\n",
            "Noun Phrases of One Sentence (one by one) :-\n",
            "natural language processing\n",
            "nlp\n",
            "computer science\n",
            "information engineering\n",
            "artificial intelligence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVVl4duSIUFj",
        "colab_type": "text"
      },
      "source": [
        "### **Part-of-speech Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y07CuOOlIUFj",
        "colab_type": "code",
        "outputId": "592d3f48-39f3-49f8-805f-8ebbd094bf29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "print(\"All Parts-of-speech - \" + str(paragraph.tags))\n",
        "print(\"\\nParts-of-speech of One Sentence (one by one) :-\")\n",
        "for words, tag in paragraph.sentences[0].tags:\n",
        "    print(words +\" - \"+ tag)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Parts-of-speech - [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('NLP', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), ('computer', 'NN'), ('science', 'NN'), ('information', 'NN'), ('engineering', 'NN'), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('natural', 'JJ'), ('languages', 'NNS'), ('In', 'IN'), ('particular', 'JJ'), ('how', 'WRB'), ('to', 'TO'), ('program', 'NN'), ('computers', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('Challenges', 'NNS'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('frequently', 'RB'), ('involve', 'VBP'), ('speech', 'NN'), ('recognition', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('understanding', 'NN'), ('and', 'CC'), ('natural', 'JJ'), ('language', 'NN'), ('generation', 'NN')]\n",
            "\n",
            "Parts-of-speech of One Sentence (one by one) :-\n",
            "Natural - JJ\n",
            "language - NN\n",
            "processing - NN\n",
            "NLP - NNP\n",
            "is - VBZ\n",
            "a - DT\n",
            "subfield - NN\n",
            "of - IN\n",
            "linguistics - NNS\n",
            "computer - NN\n",
            "science - NN\n",
            "information - NN\n",
            "engineering - NN\n",
            "and - CC\n",
            "artificial - JJ\n",
            "intelligence - NN\n",
            "concerned - VBN\n",
            "with - IN\n",
            "the - DT\n",
            "interactions - NNS\n",
            "between - IN\n",
            "computers - NNS\n",
            "and - CC\n",
            "human - JJ\n",
            "natural - JJ\n",
            "languages - NNS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpXNsijUIUFm",
        "colab_type": "text"
      },
      "source": [
        "### **Words Inflection and Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyK4F1alIUFq",
        "colab_type": "code",
        "outputId": "ea20a2e0-b635-4651-93eb-5e802fb2111c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "word = paragraph.sentences[0].words[8]\n",
        "print(\"Actual Word - \" + word)\n",
        "print(\"Singular Form - \" + word.singularize())\n",
        "\n",
        "#Alternative - use Word() method\n",
        "word = Word(\"Language\")\n",
        "print(\"\\nActual Word - \" + word)\n",
        "print(\"Singular Form - \" + word.pluralize())\n",
        "\n",
        "word = Word(\"\")\n",
        "print(\"\\nActual Word - \" + word)\n",
        "print(\"Lemmatize Form - \" + word.lemmatize())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Word - linguistics\n",
            "Singular Form - linguistic\n",
            "\n",
            "Actual Word - Language\n",
            "Singular Form - Languages\n",
            "\n",
            "Actual Word - gone\n",
            "Lemmatize Form - go\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWQnkSfXIUFw",
        "colab_type": "code",
        "outputId": "cca92718-01b0-414e-9af1-3d12187f6163",
        "colab": {}
      },
      "source": [
        "from textblob import Word\n",
        "w = Word('Platform')\n",
        "w.pluralize()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Platforms'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFKCAC4dIUFz",
        "colab_type": "code",
        "outputId": "bc25a1da-a0b4-4ebd-fcfa-ed0e13944b6c",
        "colab": {}
      },
      "source": [
        "for word,pos in blob.tags:\n",
        "    if pos == 'NN':\n",
        "        print (word.pluralize())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "platforms\n",
            "sciences\n",
            "communities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRVmmUGVIUF3",
        "colab_type": "code",
        "outputId": "0fc84daa-d14b-4aed-bc83-b0cdfdacca19",
        "colab": {}
      },
      "source": [
        "w = Word('running')\n",
        "w.lemmatize(\"v\") ## v here represents verb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'run'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd32_CsFIUF6",
        "colab_type": "text"
      },
      "source": [
        "# N-GRAMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vEoFlLCIUF7",
        "colab_type": "code",
        "outputId": "ecd10cbf-410d-4d1f-99f2-0fe420cfba24",
        "colab": {}
      },
      "source": [
        "for ngram in blob.ngrams(2):\n",
        "    print (ngram)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Analytics', 'Vidhya']\n",
            "['Vidhya', 'is']\n",
            "['is', 'a']\n",
            "['a', 'great']\n",
            "['great', 'platform']\n",
            "['platform', 'to']\n",
            "['to', 'learn']\n",
            "['learn', 'data']\n",
            "['data', 'science']\n",
            "['science', 'It']\n",
            "['It', 'helps']\n",
            "['helps', 'community']\n",
            "['community', 'through']\n",
            "['through', 'blogs']\n",
            "['blogs', 'hackathons']\n",
            "['hackathons', 'discussions']\n",
            "['discussions', 'etc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygf37TKZIUF-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LjLVC43IUF_",
        "colab_type": "text"
      },
      "source": [
        "Sentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.\n",
        "\n",
        "The sentiment function of textblob returns two properties, polarity, and subjectivity.\n",
        "\n",
        "Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].\n",
        "\n",
        "Let’s check the sentiment of our blob."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rbx_wogIUGA",
        "colab_type": "code",
        "outputId": "9ee1f76d-94e7-41f6-c3e8-a96b75332afc",
        "colab": {}
      },
      "source": [
        "print(blob)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytics Vidhya is a great platform to learn data science. \n",
            " It helps community through blogs, hackathons, discussions,etc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5djPIKTWIUGO",
        "colab_type": "code",
        "outputId": "d6d93d4d-a84a-4eb0-aa67-06761843d979",
        "colab": {}
      },
      "source": [
        "blob.sentiment"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.8, subjectivity=0.75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxeIbqZxIUGT",
        "colab_type": "text"
      },
      "source": [
        "We can see that polarity is 0.8, which means that the statement is positive and 0.75 subjectivity refers that mostly it is a public opinion and not a factual information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkg_giNVIUGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text='''The titular threat of The Blob has always struck me as the ultimate movie\n",
        "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
        "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
        "describes it--\"assimilating flesh on contact.\n",
        "Snide comparisons to gelatin be damned, it's a concept with the most\n",
        "devastating of potential consequences, not unlike the grey goo scenario\n",
        "proposed by technological theorists fearful of\n",
        "artificial intelligence run rampant.'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWNz9y3hIUGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blob1=TextBlob(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SmjAyIUIUGc",
        "colab_type": "code",
        "outputId": "8aafc356-4da7-4963-9a94-34c0bd0483db",
        "colab": {}
      },
      "source": [
        "print(blob1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The titular threat of The Blob has always struck me as the ultimate movie\n",
            "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
            "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
            "describes it--\"assimilating flesh on contact.\n",
            "Snide comparisons to gelatin be damned, it's a concept with the most\n",
            "devastating of potential consequences, not unlike the grey goo scenario\n",
            "proposed by technological theorists fearful of\n",
            "artificial intelligence run rampant.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5lluXlKIUGi",
        "colab_type": "code",
        "outputId": "029caace-89d7-4ab7-a818-d8aefeae8ede",
        "colab": {}
      },
      "source": [
        "blob1.sentiment"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=-0.1590909090909091, subjectivity=0.6931818181818182)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeBD8H0vIUGp",
        "colab_type": "text"
      },
      "source": [
        "# SPELLING CORRECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drRu1ro5IUGq",
        "colab_type": "code",
        "outputId": "c817dba3-6cec-4f08-9c45-d5e106d9920a",
        "colab": {}
      },
      "source": [
        "blob = TextBlob('Analytics Vidhya is a gret platfrm to learn data scence')\n",
        "blob.correct()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"Analytics Vidhya is a great platform to learn data science\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcHH5pIYIUGt",
        "colab_type": "text"
      },
      "source": [
        "# CREATING A SHORT SUMMARY OF TEXT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIZ0FxkJIUGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6wLRMfDIUG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blob = TextBlob('Analytics Vidhya is a thriving community for data driven industry. This platform allows \\\n",
        "people to know more about analytics from its articles, Q&A forum, and learning paths. Also, we help \\\n",
        "professionals & amateurs to sharpen their skillsets by providing a platform to participate in Hackathons.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQPyZnnKkmck",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc1HqOOAIUHA",
        "colab_type": "code",
        "outputId": "02ae5d1f-ff63-4c53-a89c-95c1b6d01606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nouns = list()\n",
        "for word, tag in blob.tags:\n",
        "    if tag == 'NN':\n",
        "        nouns.append(word.lemmatize())\n",
        "\n",
        "print (\"This text is about...\")\n",
        "for item in random.sample(nouns, 5):\n",
        "    word = Word(item)\n",
        "    print (word.pluralize())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**********************************************************************\n",
            "  Resource \u001b[93mwordnet\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('wordnet')\n",
            "  \u001b[0m\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MissingCorpusError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_wordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FILEMAP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0e2077644c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnouns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"This text is about...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingCorpusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9F0QfCUIUHG",
        "colab_type": "code",
        "outputId": "be2b565a-51d6-4a55-8470-22d840d8270e",
        "colab": {}
      },
      "source": [
        "nouns = list()\n",
        "for word, tag in blob.tags:\n",
        "    if tag == 'NN':\n",
        "        print(word, word.lemmatize())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "community community\n",
            "industry industry\n",
            "platform platform\n",
            "forum forum\n",
            "platform platform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0BuN2y0IUHI",
        "colab_type": "code",
        "outputId": "9d1042a9-c4b4-45f2-e860-0c0974e7d9b0",
        "colab": {}
      },
      "source": [
        "type(nouns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NC-oeNNIUHK",
        "colab_type": "text"
      },
      "source": [
        "# TRANSLATION AND LANGUAGE DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNcrw0FZIUHL",
        "colab_type": "code",
        "outputId": "af8cdc80-5ee0-4074-98a9-eff3574dc12b",
        "colab": {}
      },
      "source": [
        "blob.detect_language()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsVpstGmIUHO",
        "colab_type": "code",
        "outputId": "ff15f1d6-d959-4310-f1a6-4bbd8624e67b",
        "colab": {}
      },
      "source": [
        "blob.translate(from_lang='en', to ='ar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"تحليلات Vidhya هو مجتمع مزدهر لصناعة تعتمد على البيانات. تتيح هذه المنصة للأشخاص معرفة المزيد عن التحليلات من مقالاتها ومنتدى الأسئلة والأجوبة ومسارات التعلم. أيضا ، نحن نساعد المحترفين والهواة على شحذ مهاراتهم من خلال توفير منصة للمشاركة في Hackathons.\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkrilL4lIUHQ",
        "colab_type": "code",
        "outputId": "d0538c9c-32e0-48a3-ceb5-e00f7d05b7d7",
        "colab": {}
      },
      "source": [
        "blob.translate(to='hi')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"एनालिटिक्स विधा डेटा संचालित उद्योग के लिए एक संपन्न समुदाय है। यह मंच लोगों को अपने लेखों, क्यू एंड ए फोरम, और सीखने के रास्तों से विश्लेषण के बारे में अधिक जानने की अनुमति देता है। इसके अलावा, हम पेशेवरों और एमेच्योर को हैकथॉन में भाग लेने के लिए एक मंच प्रदान करके अपने कौशल को तेज करने में मदद करते हैं।\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOLB6b0JIUHW",
        "colab_type": "text"
      },
      "source": [
        "# TEXT CLASSIFICATION USING TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPgersvkIUHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = [\n",
        "('Tom Holland is a terrible spiderman.','pos'),\n",
        "('a terrible Javert (Russell Crowe) ruined Les Miserables for me...','pos'),\n",
        "('The Dark Knight Rises is the greatest superhero movie ever!','neg'),\n",
        "('Fantastic Four should have never been made.','pos'),\n",
        "('Wes Anderson is my favorite director!','neg'),\n",
        "('Captain America 2 is pretty awesome.','neg'),\n",
        "('Let\\s pretend \"Batman and Robin\" never happened..','pos'),\n",
        "]\n",
        "testing = [\n",
        "('Superman was never an interesting character.','pos'),\n",
        "('Fantastic Mr Fox is an awesome film!','neg'),\n",
        "('Dragonball Evolution is simply terrible!!','pos')\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEMjM9_IUHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import classifiers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kkOgeSMIUHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = classifiers.NaiveBayesClassifier(training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr8_wkEaIUHe",
        "colab_type": "text"
      },
      "source": [
        "As you can see above, we have passed the training data into the classifier.\n",
        "\n",
        "Note that here we have used Naive Bayes classifier, but TextBlob also offers Decision tree classifier which is as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K1HadizIUHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt_classifier = classifiers.DecisionTreeClassifier(training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr8CwfDZIUHh",
        "colab_type": "code",
        "outputId": "1214719b-725c-49d2-94f7-11cc3becdfac",
        "colab": {}
      },
      "source": [
        "print (classifier.accuracy(testing))\n",
        "classifier.show_informative_features(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "Most Informative Features\n",
            "            contains(is) = True              neg : pos    =      2.9 : 1.0\n",
            "             contains(a) = False             neg : pos    =      1.8 : 1.0\n",
            "      contains(terrible) = False             neg : pos    =      1.8 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Rx9aCCIUHo",
        "colab_type": "code",
        "outputId": "2f0778ed-b882-4a7c-9522-20faa9aa0862",
        "colab": {}
      },
      "source": [
        "blob = TextBlob('the weather is terrible!', classifier=classifier)\n",
        "print (blob.classify())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCzs--J6IUHq",
        "colab_type": "text"
      },
      "source": [
        "So, based on the training on the above dataset, our classifier has provided us the right result.\n",
        "\n",
        "Note that here we could have done some preprocessing and data cleaning but here my aim was to give you an intuition that how we can do text classification using TextBlob."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HosZ46A_IUHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}